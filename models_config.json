{
  "models": {
    "qwen3-30b-instruct": {
      "name": "unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF",
      "type": "instruction",
      "size": "30B",
      "quantization": "4bit",
      "description": "Large instruction-following model for complex tasks. Effective context: ~128k tokens, maximum supported: 256k tokens",
      "memory_estimate_gb": 18,
      "recommended_gpus": 2,
      "chat_template": "qwen3",
      "trust_remote_code": true,
      "effective_context_tokens": 131072,
      "max_context_tokens": 262144,
      "default_params": {
        "temperature": 0.7,
        "max_tokens": 8192,
        "top_p": 0.9
      }
    },
    "phi-4-reasoning": {
      "name": "unsloth/Phi-4-reasoning-GGUF",
      "type": "instruction",
      "size": "15B", 
      "quantization": "4bit",
      "description": "Medium reasoning model, good balance. Effective context: ~64k tokens, maximum supported: 256k tokens",
      "memory_estimate_gb": 8,
      "recommended_gpus": 1,
      "chat_template": "qwen3",
      "trust_remote_code": true,
      "effective_context_tokens": 65536,
      "max_context_tokens": 262144,
      "default_params": {
        "temperature": 0.7,
        "max_tokens": 8192,
        "top_p": 0.9
      }
    },
    "qwen3-8b-instruct": {
      "name": "lm-kit/qwen-3-8b-instruct-gguf",
      "type": "instruction", 
      "size": "7B",
      "quantization": "8bit",
      "description": "Qwen3-7B instruction-tuned GGUF. Effective context: ~32k tokens, maximum supported: 131k tokens",
      "memory_estimate_gb": 5,
      "recommended_gpus": 1,
      "chat_template": "qwen3",
      "trust_remote_code": true,
      "effective_context_tokens": 32768,
      "max_context_tokens": 262144,
      "default_params": {
        "temperature": 0.7,
        "max_tokens": 8192,
        "top_p": 0.9
      }
    },
    "phi-4-mini": {
      "name": "unsloth/Phi-4-mini-instruct-GGUF",
      "type": "instruction", 
      "size": "4B",
      "quantization": "4bit",
      "description": "Fast instruction-following model for simple tasks. Effective context: ~16k tokens, maximum supported: 256k tokens",
      "memory_estimate_gb": 2,
      "recommended_gpus": 1,
      "chat_template": "qwen3",
      "trust_remote_code": true,
      "effective_context_tokens": 16384,
      "max_context_tokens": 262144,
      "default_params": {
        "temperature": 0.7,
        "max_tokens": 8192,
        "top_p": 0.9
      }
    },
    "qwen3-coder-14b": {
      "name": "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
      "type": "coder",
      "size": "30B",
      "quantization": "4bit", 
      "description": "Specialized for code generation and programming. Effective context: ~128k tokens, maximum supported: 256k tokens",
      "memory_estimate_gb": 18,
      "recommended_gpus": 2,
      "chat_template": "qwen3",
      "trust_remote_code": true,
      "effective_context_tokens": 131072,
      "max_context_tokens": 262144,
      "default_params": {
        "temperature": 0.3,
        "max_tokens": 8192,
        "top_p": 0.95
      }
    }
  },
  "active_model": "qwen3-8b-instruct",
  "download_path": "./models",
  "cache_dir": "./cache",
  "server": {
    "host": "127.0.0.1",
    "port": 8080,
    "log_level": "INFO",
    "use_jinja_template": true,
    "template_dir": "templates"
  }
} 